import os
import numpy as np
import argparse
from DDQN import DDQN
from DQN import DQN
from utils import plot_learning_curve, create_directory
from env import envi
import matplotlib.pyplot as plt
plt.style.use('seaborn')

parser = argparse.ArgumentParser()
parser.add_argument('--max_episodes', type=int, default=200)
parser.add_argument('--ckpt_dir', type=str, default='./checkpoints/DDQN/')
parser.add_argument('--reward_path', type=str, default='./img/reward.png')
parser.add_argument('--action_path', type=str, default='./img/action.png')
parser.add_argument('--state_path', type=str, default='./img/state.png')

args = parser.parse_args()


def main():
    env = envi()
    env2 = envi()
    agent = (DDQN(alpha=0.0003, state_dim=env.observation_space, action_dim=env.action_space,
                  fc1_dim=256, fc2_dim=256, ckpt_dir=args.ckpt_dir, gamma=0.99, tau=0.01, epsilon=1.0,
                  eps_end=0.05, eps_dec=5e-4, max_size=1000000, batch_size=256))
    agent2 = DQN(alpha=0.0003, state_dim=env.observation_space, action_dim=env.action_space,
                  fc1_dim=256, fc2_dim=256, ckpt_dir=args.ckpt_dir, gamma=0.99, tau=0.01, epsilon=1.0,
                  eps_end=0.05, eps_dec=5e-4, max_size=1000000, batch_size=256)
    create_directory(args.ckpt_dir, sub_dirs=['Q_eval', 'Q_target'])
    states, rewards, actions, eps_history = [], [], [], []
    states2, rewards2, actions2, eps_history2 = [], [], [], []
    statesAllOfIL, statesAllOfRP, rewardsOfAll = [], [], []
    cur_state = env.reset()
    cur_state2 = env2.reset()

    for episode in range(args.max_episodes):
        statesOfIL = [cur_state[1],cur_state2[1]]
        statesAllOfIL.append(statesOfIL)
        statesOfRP = [cur_state[2],cur_state2[2]]
        statesAllOfRP.append(statesOfRP)
        states.append(cur_state)
        states2.append(cur_state2)
         # 在这里智能体独自用自己的状态去产生行为
        action = agent.choose_action(cur_state, isTrain=True)
        action2 = agent2.choose_action(cur_state2, isTrain=True)
        next_state, reward, done, info = env.stepMixDifferEta_c_n(action, 0.4, .1)
        next_state2, reward2, done2, info2 = env2.stepMixDifferEta_c_n(action2, 0.4, .3)
        agent.remember(cur_state, action, reward, next_state, done)
        agent2.remember(cur_state2, action2, reward2, next_state2, done2)
        agent.learn()
        agent2.learn()
        cur_state = next_state
        cur_state2 = next_state2
        #print(np.mean(reward))

        #print(rewar/max_timesteps)
        rewards.append(reward)
        rewards2.append(reward2)

        rewardsOfPerTime = [reward,reward2]
        rewardsOfAll.append(rewardsOfPerTime)
        actions.append(action)
        actions2.append(action2)
        eps_history.append(agent.epsilon)

        if (episode + 1) % 50 == 0:
            agent.save_models(episode + 1)
            agent2.save_models(episode + 1)

    episodes = [i for i in range(args.max_episodes)]

    plt.figure(0)
    observations = np.array(statesAllOfIL)
    plt.plot((observations[:, :]))
    plt.xlabel('Time step')
    plt.ylabel('Ratio of ' r'$IL$ ' 'devices in different algorithms')
    plt.legend([r'$DQN$',  r'$DDPV$'], loc='upper right')
    plt.savefig('./img/diffDRL/stateOfIL.png')
    plt.show()

    plt.figure(0)
    observations = np.array(statesAllOfRP)
    plt.plot((observations[:, :]))
    plt.xlabel('Time step')
    plt.ylabel('Ratio of ' r'$RP$ ' 'devices in different algorithms')
    plt.legend([r'$DQN$',  r'$DDPV$'], loc='lower right')
    plt.savefig('./img/diffDRL/stateOfRP.png')
    plt.show()

    plt.figure(0)
    observations = np.array(states)
    plt.plot((observations[:, :]))
    plt.xlabel('Time step')
    plt.ylabel('Ratio of devices in different algorithms')
    plt.legend([r'$SL$', r'$IL$', r'$RP$', r'$DL$'], loc='center right')
    plt.savefig('./img/diffDRL/state.png')
    plt.show()

    plt.figure(0)
    observations = np.array(states2)
    plt.plot((observations[:, :]))
    plt.xlabel('Time step')
    plt.ylabel('Ratio of devices in different algorithms')
    plt.legend([r'$SL$', r'$IL$', r'$RP$', r'$DL$'], loc='center right')
    plt.savefig('./img/diffDRL/state2.png')
    plt.show()


    plt.figure(0)
    rewardsOfAll = np.array(rewardsOfAll)
    plt.plot((rewardsOfAll[:, :]))
    plt.xlabel('Time step')
    plt.ylabel('Rewards')
    plt.legend([r'$DQN$',  r'$DDPV$'], loc='lower right')
    plt.savefig('./img/diffDRL/rewardsOfAll.png')
    plt.show()


    actionDesc = ['(0,0,0)', '(0,0,1)', '(0,1,0)', '(0,1,1)',
                  '(1,0,0)', '(1,0,1)', '(1,1,0)', '(1,1,1)']
    actionCount = [0,0,0,0,0,0,0,0]
    actionCount2 = [0, 0, 0, 0, 0, 0, 0, 0]
    for i in range(len(actions)):
        if i > 80:
            # print(actions[i])
            actionCount[actions[i]] = actionCount[actions[i]] + 1
            actionCount2[actions2[i]] = actionCount2[actions2[i]] + 1

    x = np.arange(len(actionDesc))  # 标签位置
    width = 0.1  # 柱状图的宽度
    fig, ax = plt.subplots()
    rects1 = ax.bar(x - width * 2, actionCount, width, label=r'$DQN$', hatch="...", color='w', edgecolor="k")
    rects2 = ax.bar(x + .02, actionCount2, width, label=r'$DDPV$', hatch="XX", color='w', edgecolor="k")
    # 为y轴、标题和x轴等添加一些文本。
    ax.set_ylabel('Statistics of actions', fontsize=12)
    ax.set_xlabel('Values of actions ' r'$e_v, e_c$ ' 'and ' r'$e_n$ ' 'in different algorithms', fontsize=12)
    # ax.set_title('标题')
    ax.set_xticks(x)
    ax.set_xticklabels(actionDesc)
    ax.legend()
    plt.savefig('./img/diffDRL/actionOfAll.png')
    plt.show()

if __name__ == '__main__':
    main()
