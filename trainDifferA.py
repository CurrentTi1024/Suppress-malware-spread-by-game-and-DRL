import os
import numpy as np
import argparse
from DDQN import DDQN
from utils import plot_learning_curve, create_directory
from env import envi
import matplotlib.pyplot as plt
plt.style.use('seaborn')

parser = argparse.ArgumentParser()
parser.add_argument('--max_episodes', type=int, default=200)
parser.add_argument('--ckpt_dir', type=str, default='./checkpoints/DDQN/')
parser.add_argument('--reward_path', type=str, default='./img/reward.png')
parser.add_argument('--action_path', type=str, default='./img/action.png')
parser.add_argument('--state_path', type=str, default='./img/state.png')
# parser.add_argument('--energy_path', type=str, default='./img/energy.png')

args = parser.parse_args()


def main():
    env = envi()
    env2 = envi()
    env3 = envi()
    env4 = envi()
    env5 = envi()
    agent = (DDQN(alpha=0.0003, state_dim=env.observation_space, action_dim=env.action_space,
                  fc1_dim=256, fc2_dim=256, ckpt_dir=args.ckpt_dir, gamma=0.99, tau=0.01, epsilon=1.0,
                  eps_end=0.05, eps_dec=5e-4, max_size=1000000, batch_size=256))
    agent2 = (DDQN(alpha=0.0003, state_dim=env2.observation_space, action_dim=env2.action_space,
                  fc1_dim=256, fc2_dim=256, ckpt_dir=args.ckpt_dir, gamma=0.99, tau=0.01, epsilon=1.0,
                  eps_end=0.05, eps_dec=5e-4, max_size=1000000, batch_size=256))
    agent3 = (DDQN(alpha=0.0003, state_dim=env3.observation_space, action_dim=env3.action_space,
                  fc1_dim=256, fc2_dim=256, ckpt_dir=args.ckpt_dir, gamma=0.99, tau=0.01, epsilon=1.0,
                  eps_end=0.05, eps_dec=5e-4, max_size=1000000, batch_size=256))
    agent4 = (DDQN(alpha=0.0003, state_dim=env3.observation_space, action_dim=env3.action_space,
                  fc1_dim=256, fc2_dim=256, ckpt_dir=args.ckpt_dir, gamma=0.99, tau=0.01, epsilon=1.0,
                  eps_end=0.05, eps_dec=5e-4, max_size=1000000, batch_size=256))
    agent5 = (DDQN(alpha=0.0003, state_dim=env3.observation_space, action_dim=env3.action_space,
                  fc1_dim=256, fc2_dim=256, ckpt_dir=args.ckpt_dir, gamma=0.99, tau=0.01, epsilon=1.0,
                  eps_end=0.05, eps_dec=5e-4, max_size=1000000, batch_size=256))

    create_directory(args.ckpt_dir, sub_dirs=['Q_eval', 'Q_target'])
    states, rewards, actions, eps_history = [], [], [], []
    states2, rewards2, actions2, eps_history2 = [], [], [], []
    states3, rewards3, actions3, eps_history3 = [], [], [], []
    states4, rewards4, actions4, eps_history4 = [], [], [], []
    states5, rewards5, actions5, eps_history5 = [], [], [], []
    statesAllOfIL, statesAllOfRP, rewardsOfAll = [], [], []
    cur_state = env.reset()
    cur_state2 = env2.reset()
    cur_state3 = env3.reset()
    cur_state4 = env4.reset()
    cur_state5 = env5.reset()

    for episode in range(args.max_episodes):
        statesOfIL = [cur_state[1],cur_state2[1],cur_state3[1],cur_state4[1],cur_state5[1]]
        statesAllOfIL.append(statesOfIL)
        statesOfRP = [cur_state[2],cur_state2[2],cur_state3[2],cur_state4[2],cur_state5[2]]
        statesAllOfRP.append(statesOfRP)
        states.append(cur_state)
        states2.append(cur_state2)
        states3.append(cur_state3)
        states4.append(cur_state4)
        states5.append(cur_state5)
         # 在这里智能体独自用自己的状态去产生行为
        action = agent.choose_action(cur_state, isTrain=True)
        action2 = agent2.choose_action(cur_state2, isTrain=True)
        action3 = agent3.choose_action(cur_state3, isTrain=True)
        action4 = agent4.choose_action(cur_state4, isTrain=True)
        action5 = agent5.choose_action(cur_state5, isTrain=True)
        next_state, reward, done, info = env.stepMixDifferA(action, 0)
        next_state2, reward2, done2, info2 = env2.stepMixDifferA(action2, 0.03)
        next_state3, reward3, done3, info3 = env2.stepMixDifferA(action3, 0.2)
        next_state4, reward4, done4, info4 = env3.stepMixDifferA(action4, 0.4)
        next_state5, reward5, done5, info5 = env3.stepMixDifferA(action5, 1)
        agent.remember(cur_state, action, reward, next_state, done)
        agent2.remember(cur_state2, action2, reward2, next_state2, done2)
        agent3.remember(cur_state3, action3, reward3, next_state3, done3)
        agent4.remember(cur_state4, action4, reward4, next_state4, done4)
        agent5.remember(cur_state5, action5, reward5, next_state5, done5)
        agent.learn()
        agent2.learn()
        agent3.learn()
        agent4.learn()
        agent5.learn()
        cur_state = next_state
        cur_state2 = next_state2
        cur_state3 = next_state3
        cur_state4 = next_state4
        cur_state5 = next_state5
        
        rewards.append(reward)
        rewards2.append(reward2)
        rewards3.append(reward3)
        rewards4.append(reward4)
        rewards4.append(reward4)

        rewardsOfPerTime = [reward,reward2,reward3,reward4, reward5]
        rewardsOfAll.append(rewardsOfPerTime)
        actions.append(action)
        actions2.append(action2)
        actions3.append(action3)
        actions4.append(action4)
        actions5.append(action5)
        eps_history.append(agent.epsilon)

        if (episode + 1) % 50 == 0:
            agent.save_models(episode + 1)
            agent2.save_models(episode + 1)
            agent3.save_models(episode + 1)
            agent4.save_models(episode + 1)
            agent5.save_models(episode + 1)

    episodes = [i for i in range(args.max_episodes)]

    plt.figure(0)
    observations = np.array(statesAllOfIL)
    plt.plot((observations[:, :]))
    plt.xlabel('Time step')
    plt.ylabel('Ratio of ' r'$IL$ ' 'devices with different ' r'$a$')
    plt.legend([r'$a$''=0.0',r'$a$''=0.03', r'$a$''=0.2', r'$a$''=0.4',r'$a$''=1.0'], loc='upper right')
    plt.savefig('./img/differA/stateOfIL.png')
    plt.show()

    plt.figure(0)
    observations = np.array(statesAllOfRP)
    plt.plot((observations[:, :]))
    plt.xlabel('Time step')
    plt.ylabel('Ratio of ' r'$RP$ ' 'devices with different ' r'$a$')
    plt.legend([r'$a$''=0.0',r'$a$''=0.03', r'$a$''=0.2', r'$a$''=0.4',r'$a$''=1.0'], loc='lower right')
    plt.savefig('./img/differA/stateOfRP.png')
    plt.show()

    plt.figure(0)
    observations = np.array(states)
    plt.plot((observations[:, :]))
    plt.xlabel('Time step')
    plt.ylabel('Ratio of devices in different states')
    plt.legend([r'$SL$', r'$IL$', r'$RP$', r'$DL$'], loc='center right')
    plt.savefig('./img/differA/state.png')
    plt.show()

    plt.figure(0)
    observations = np.array(states2)
    plt.plot((observations[:, :]))
    plt.xlabel('Time step')
    plt.ylabel('Ratio of devices in different states')
    plt.legend([r'$SL$', r'$IL$', r'$RP$', r'$DL$'], loc='center right')
    plt.savefig('./img/differA/state2.png')
    plt.show()

    plt.figure(0)
    observations = np.array(states3)
    plt.plot((observations[:, :]))
    # plt.title('Average state distribution mu for each episode')
    plt.xlabel('Time step')
    plt.ylabel('Ratio of devices in different states')
    plt.legend([r'$SL$', r'$IL$', r'$RP$', r'$DL$'], loc='center right')
    plt.savefig('./img/differA/state3.png')
    plt.show()

    plt.figure(0)
    observations = np.array(states4)
    plt.plot((observations[:, :]))
    # plt.title('Average state distribution mu for each episode')
    plt.xlabel('Time step')
    plt.ylabel('Ratio of devices in different states')
    plt.legend([r'$SL$', r'$IL$', r'$RP$', r'$DL$'], loc='center right')
    plt.savefig('./img/differA/state4.png')
    plt.show()

    plt.figure(0)
    observations = np.array(states5)
    plt.plot((observations[:, :]))
    # plt.title('Average state distribution mu for each episode')
    plt.xlabel('Time step')
    plt.ylabel('Ratio of devices in different states')
    plt.legend([r'$SL$', r'$IL$', r'$RP$', r'$DL$'], loc='center right')
    plt.savefig('./img/differA/state5.png')
    plt.show()


    plt.figure(0)
    rewardsOfAll = np.array(rewardsOfAll)
    plt.plot((rewardsOfAll[:, :]))
    # plt.title('Average state distribution mu for each episode')
    plt.xlabel('Time step')
    plt.ylabel('Reward for IIoT networks of model ' r'$IIPV$ ' 'with different ' r'$a$')
    plt.legend([r'$a$''=0.0',r'$a$''=0.03', r'$a$''=0.2', r'$a$''=0.4',r'$a$''=1.0'], loc='lower right')
    plt.savefig('./img/differA/rewardsOfAll.png')
    plt.show()

    actionDesc = ['(0,0,0)', '(0,0,1)', '(0,1,0)', '(0,1,1)',
                  '(1,0,0)', '(1,0,1)', '(1,1,0)', '(1,1,1)']
    actionCount = [0, 0, 0, 0, 0, 0, 0, 0]
    actionCount2 = [0, 0, 0, 0, 0, 0, 0, 0]
    actionCount3 = [0, 0, 0, 0, 0, 0, 0, 0]
    actionCount4 = [0, 0, 0, 0, 0, 0, 0, 0]
    actionCount5 = [0, 0, 0, 0, 0, 0, 0, 0]
    for i in range(len(actions)):
        if i > 80:
            # print(actions[i])
            actionCount[actions[i]] = actionCount[actions[i]] + 1
            actionCount2[actions2[i]] = actionCount2[actions2[i]] + 1
            actionCount3[actions3[i]] = actionCount3[actions3[i]] + 1
            actionCount4[actions4[i]] = actionCount4[actions4[i]] + 1
            actionCount5[actions5[i]] = actionCount5[actions5[i]] + 1

    x = np.arange(len(actionDesc))  # 标签位置
    width = 0.1  # 柱状图的宽度
    fig, ax = plt.subplots()
    rects1 = ax.bar(x - width * 2, actionCount, width, label=r'$a$''=0.0', hatch="...", color='w', edgecolor="k")
    rects2 = ax.bar(x - width + 0.01, actionCount2, width, label=r'$a$''=0.03', hatch="XX", color='w', edgecolor="k")
    rects3 = ax.bar(x + 0.02, actionCount3, width, label=r'$a$''=0.2', hatch="++", color='w', edgecolor="k")
    rects4 = ax.bar(x + width + 0.03, actionCount4, width, label=r'$a$''=0.4', hatch="oo", color='w', edgecolor="k")
    rects5 = ax.bar(x + width * 2 + 0.04, actionCount5, width, label=r'$a$''=1.0', hatch="**", color='w', edgecolor="k")
    # 为y轴、标题和x轴等添加一些文本。
    ax.set_ylabel('Statistics of actions', fontsize=12)
    ax.set_xlabel('Values of actions ' r'$e_v, e_c$ ' 'and ' r'$e_n$', fontsize=12)
    ax.set_xticks(x)
    ax.set_xticklabels(actionDesc)
    ax.legend()
    plt.savefig('./img/differA/actionOfAll.png')
    plt.show()

    

if __name__ == '__main__':
    main()
