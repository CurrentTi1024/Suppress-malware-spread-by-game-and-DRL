import os
import numpy as np
import argparse
from DDQN import DDQN
from utils import plot_learning_curve, create_directory
from env import envi
import matplotlib.pyplot as plt
plt.style.use('seaborn')

parser = argparse.ArgumentParser()
parser.add_argument('--max_episodes', type=int, default=200)
parser.add_argument('--ckpt_dir', type=str, default='./checkpoints/DDQN/')
parser.add_argument('--reward_path', type=str, default='./img/reward.png')
parser.add_argument('--action_path', type=str, default='./img/action.png')
parser.add_argument('--state_path', type=str, default='./img/state.png')

args = parser.parse_args()


def main():
    env = envi()
    agent = (DDQN(alpha=0.0003, state_dim=env.observation_space, action_dim=env.action_space,
                  fc1_dim=256, fc2_dim=256, ckpt_dir=args.ckpt_dir, gamma=0.99, tau=0.01, epsilon=1.0,
                  eps_end=0.05, eps_dec=5e-4, max_size=1000000, batch_size=256))

    create_directory(args.ckpt_dir, sub_dirs=['Q_eval', 'Q_target'])
    states, rewards, actions, eps_history = [], [], [], []
    cur_state = env.reset()

    for episode in range(args.max_episodes):
        states.append(cur_state)
         # 智能体独自用自己的状态去产生行为
        action = agent.choose_action(cur_state, isTrain=True)
        next_state, reward, done, info = env.stepMix(action)
        agent.remember(cur_state, action, reward, next_state, done)
        agent.learn()
        cur_state = next_state
        
        rewards.append(reward)
        actions.append(action)
        eps_history.append(agent.epsilon)

        if (episode + 1) % 50 == 0:
            agent.save_models(episode + 1)

    episodes = [i for i in range(args.max_episodes)]
    plot_learning_curve(episodes, rewards, '', 'Reward for IIoT networks of model ' r'$IIPV$', args.reward_path)
    

    plt.figure(0)
    observations = np.array(states)
    plt.plot((observations[:, :]))
    plt.xlabel('Time step')
    plt.ylabel('Ratio of devices in different states')
    plt.legend([r'$SL$', r'$IL$', r'$RP$', r'$DL$'], loc='center right')
    plt.savefig('./img/state.png')
    plt.show()


    actionDesc = ['(0,0,0)', '(0,0,1)', '(0,1,0)', '(0,1,1)',
                  '(1,0,0)', '(1,0,1)', '(1,1,0)', '(1,1,1)']
    actionCount = [0, 0, 0, 0, 0, 0, 0, 0]
    for i in range(len(actions)):
        if i > 80:
            actionCount[actions[i]] = actionCount[actions[i]] + 1

    x = np.arange(len(actionDesc))  # 标签位置
    width = 0.3  # 柱状图的宽度
    fig, ax = plt.subplots()
    rects1 = ax.bar(x , actionCount, width, hatch="...", color='w', edgecolor="k")
    ax.set_ylabel('Statistics of actions', fontsize=12)
    ax.set_xlabel('Values of actions ' r'$e_v, e_c$ ' 'and ' r'$e_n$', fontsize=12)
    ax.set_xticks(x)
    ax.set_xticklabels(actionDesc)
    ax.legend()
    plt.savefig('./img/action.png')
    plt.show()


if __name__ == '__main__':
    main()
