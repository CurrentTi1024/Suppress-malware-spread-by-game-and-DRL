import numpy as np

T = 100
N_s = 1000
delta_t = T/N_s
k_avg = 5  #avg degree

beta = 0.3  
eta_c = 0.4 
eta_n = 0.3  
l = 0.01 
d = 0.0001 

# cost
p_t = .2  
c_c = .25  
c_n = .2  
kk = 100
m = .5  
r = 1  

a = 0.1  # patch choose rate

e_v = 1  # 攻击者
e_c = 1  # conter patch
e_n = 1  # local patch


lambd = .8  

episodes = 1000

epsilon = .1
gamma = .5
omega_Q = .55

class envi():
    def __init__(self):
        self.observation_space = 4 
        self.observation = np.array([.9,.1,.0,.0])
        self.action_space = 8 
        self.n_actions = 8    
        self.n_features = 8

    
    def reset(self):
        # return np.array([.9,.1,.0,.0])
        # return np.array([.25,.25,.25,.25])
        return np.array([.99,.01,.0,.0])
    
    
    def stepNoPatch(self):
        observation_ = np.empty(4)
    
        
        observation_[0] = self.observation[0] + (-k_avg*beta*self.observation[0]*self.observation[1] - (1 - a)* eta_n *self.observation[0] - eta_c * a * self.observation[0] + l * observation_[2])
        observation_[1] = self.observation[1] + (k_avg*beta*self.observation[0]*self.observation[1] - (1 - a)* eta_n *self.observation[1] - eta_c * a * self.observation[1] - d * self.observation[1])
        observation_[2] = self.observation[2] + ((1 - a)* eta_n * (self.observation[0] + self.observation[1]) + e_c*eta_c * a * (self.observation[0] + self.observation[1]) - l * observation_[2])
        observation_[3] = self.observation[3] + (d*self.observation[1])
    
        reward = -(p_t * self.observation[1] + c_c * (self.observation[0] + self.observation[1]) + kk * self.observation[3] - r * self.observation[2] + m * self.observation[3])
    
        self.observation = observation_
        return observation_, reward, False, "info"
    
    # 混合补丁分发模式下网络的状态
    def stepMix(self, action):
    
        observation_ = np.empty(4)
    
        
    	# action是二进制000~111的十进制数，即取值范围0-7。涵盖了三种行为的所有组合模式
        e_v = action >> 2
        e_c = (action >> 1) % 2
        e_n = action % 2
        print(e_v, e_c, e_n)



        # 有k加自己有action ----IIPV
        observation_[0] = self.observation[0] + (- e_v*k_avg*beta*self.observation[0]*self.observation[1] - e_n*(1 - a)* eta_n *self.observation[0] - e_c*eta_c * a * self.observation[0] + l * observation_[2])
        observation_[1] = self.observation[1] + (e_v* k_avg*beta*self.observation[0]*self.observation[1] - e_n*(1 - a)* eta_n *self.observation[1] - e_c*eta_c * a * self.observation[1] - d * self.observation[1])
        observation_[2] = self.observation[2] + (e_n*(1 - a)* eta_n * (self.observation[0] + self.observation[1]) + e_c*eta_c * a * (self.observation[0] + self.observation[1]) - l * observation_[2])
        observation_[3] = self.observation[3] + (d*self.observation[1])
        reward = -(p_t * self.observation[1] + c_n * (1 - a) * (self.observation[0] + self.observation[1]) + c_c * a *
                (self.observation[0] + self.observation[1]) + kk * self.observation[3] - r * self.observation[2] + m * self.observation[3])
    
        
        self.observation = observation_
        return observation_, reward, False, "info"


    # 不同beta下的IIPV（混合式补丁分发）
    def stepMixDifferBeta(self, action, beta):
        # observation_ = np.zeros(4)
        observation_ = np.empty(4)
    
        e_v = action >> 2
        e_c = (action >> 1) % 2
        e_n = action % 2
        # print(beta)
        # 有k加自己有action ----IIPV
        observation_[0] = self.observation[0] + (- e_v*k_avg*beta*self.observation[0]*self.observation[1] - e_n*(1 - a)* eta_n *self.observation[0] - e_c*eta_c * a * self.observation[0] + l * observation_[2])
        observation_[1] = self.observation[1] + (e_v* k_avg*beta*self.observation[0]*self.observation[1] - e_n*(1 - a)* eta_n *self.observation[1] - e_c*eta_c * a * self.observation[1] - d * self.observation[1])
        observation_[2] = self.observation[2] + (e_n*(1 - a)* eta_n * (self.observation[0] + self.observation[1]) + e_c*eta_c * a * (self.observation[0] + self.observation[1]) - l * observation_[2])
        observation_[3] = self.observation[3] + (d*self.observation[1])
        reward = -(p_t * self.observation[1] + c_n * (1 - a) * (self.observation[0] + self.observation[1]) + c_c * a *
                (self.observation[0] + self.observation[1]) + kk * self.observation[3] - r * self.observation[2] + m * self.observation[3])
    
        self.observation = observation_
        return observation_, reward, False, "info"

# 不同eta_c和eta_n下的IIPV（混合式补丁分发）
    def stepMixDifferEta_c_n(self, action, eta_c, eta_n):
        # observation_ = np.zeros(4)
        observation_ = np.empty(4)
    
        e_v = action >> 2
        e_c = (action >> 1) % 2
        e_n = action % 2
        # print(beta)
        # 有k加自己有action ----IIPV
        observation_[0] = self.observation[0] + (- e_v*k_avg*beta*self.observation[0]*self.observation[1] - e_n*(1 - a)* eta_n *self.observation[0] - e_c*eta_c * a * self.observation[0] + l * observation_[2])
        observation_[1] = self.observation[1] + (e_v* k_avg*beta*self.observation[0]*self.observation[1] - e_n*(1 - a)* eta_n *self.observation[1] - e_c*eta_c * a * self.observation[1] - d * self.observation[1])
        observation_[2] = self.observation[2] + (e_n*(1 - a)* eta_n * (self.observation[0] + self.observation[1]) + e_c*eta_c * a * (self.observation[0] + self.observation[1]) - l * observation_[2])
        observation_[3] = self.observation[3] + (d*self.observation[1])
        reward = -(p_t * self.observation[1] + c_n * (1 - a) * (self.observation[0] + self.observation[1]) + c_c * a *
                (self.observation[0] + self.observation[1]) + kk * self.observation[3] - r * self.observation[2] + m * self.observation[3])
    
        self.observation = observation_
        return observation_, reward, False, "info"

# 不同a下的IIPV（混合式补丁分发）
    def stepMixDifferA(self, action, a):
        # observation_ = np.zeros(4)
        observation_ = np.empty(4)
    
        e_v = action >> 2
        e_c = (action >> 1) % 2
        e_n = action % 2
        # 有k加自己有action ----IIPV
        observation_[0] = self.observation[0] + (- e_v*k_avg*beta*self.observation[0]*self.observation[1] - e_n*(1 - a)* eta_n *self.observation[0] - e_c*eta_c * a * self.observation[0] + l * observation_[2])
        observation_[1] = self.observation[1] + (e_v* k_avg*beta*self.observation[0]*self.observation[1] - e_n*(1 - a)* eta_n *self.observation[1] - e_c*eta_c * a * self.observation[1] - d * self.observation[1])
        observation_[2] = self.observation[2] + (e_n*(1 - a)* eta_n * (self.observation[0] + self.observation[1]) + e_c*eta_c * a * (self.observation[0] + self.observation[1]) - l * observation_[2])
        observation_[3] = self.observation[3] + (d*self.observation[1])
        reward = -(p_t * self.observation[1] + c_n * (1 - a) * (self.observation[0] + self.observation[1]) + c_c * a *
                (self.observation[0] + self.observation[1]) + kk * self.observation[3] - r * self.observation[2] + m * self.observation[3])
    
        self.observation = observation_
        return observation_, reward, False, "info"


# 不同代价下的IIPV（混合式补丁分发）
    def stepMixDifferCost(self, action, p_t, c_c, c_n, kk, m, r):
        # observation_ = np.zeros(4)
        observation_ = np.empty(4)
    
        e_v = action >> 2
        e_c = (action >> 1) % 2
        e_n = action % 2
        # 有k加自己有action ----IIPV
        observation_[0] = self.observation[0] + (- e_v*k_avg*beta*self.observation[0]*self.observation[1] - e_n*(1 - a)* eta_n *self.observation[0] - e_c*eta_c * a * self.observation[0] + l * observation_[2])
        observation_[1] = self.observation[1] + (e_v* k_avg*beta*self.observation[0]*self.observation[1] - e_n*(1 - a)* eta_n *self.observation[1] - e_c*eta_c * a * self.observation[1] - d * self.observation[1])
        observation_[2] = self.observation[2] + (e_n*(1 - a)* eta_n * (self.observation[0] + self.observation[1]) + e_c*eta_c * a * (self.observation[0] + self.observation[1]) - l * observation_[2])
        observation_[3] = self.observation[3] + (d*self.observation[1])
        reward = -(p_t * self.observation[1] + c_n * (1 - a) * (self.observation[0] + self.observation[1]) + c_c * a *
                (self.observation[0] + self.observation[1]) + kk * self.observation[3] - r * self.observation[2] + m * self.observation[3])
    
        self.observation = observation_
        return observation_, reward, False, "info"

# 只支持邻居补丁分发的IIPV（分散式补丁分发）
    def stepOnlyN(self, action):
    
        # observation_ = np.zeros(4)
        observation_ = np.empty(4)
    
        e_v = action >> 2
        e_c = (action >> 1) % 2
        e_n = action % 2
        # print(e_v, e_c, e_n)
    
        # 单边缘网络
        observation_[0] = self.observation[0] - e_v * k_avg * beta * self.observation[0] * self.observation[1] - e_n * eta_n * self.observation[0] + l * observation_[2]
        observation_[1] = self.observation[1] + e_v * k_avg * beta * self.observation[0] * self.observation[1] - e_n * eta_n * self.observation[1] - d * self.observation[1]
        observation_[2] = self.observation[2] + e_n * eta_n * (self.observation[0] + self.observation[1]) - l * observation_[2]
        observation_[3] = self.observation[3] + d * self.observation[1]
    
        reward = -(p_t * self.observation[1] + c_n * (self.observation[0] + self.observation[1]) + kk * self.observation[3] - r * self.observation[2] + m * self.observation[3])
    
        self.observation = observation_
        return observation_, reward, False, "info"

# 只支持中心补丁分发的IIPV（集中式补丁分发）
    def stepOnlyC(self, action):
    
        # observation_ = np.zeros(4)
        observation_ = np.empty(4)
    
        e_v = action >> 2
        e_c = (action >> 1) % 2
        e_n = action % 2
        # print(e_v, e_c, e_n)
    
        # 中心
        observation_[0] = self.observation[0] - e_v * k_avg * beta * self.observation[0] * self.observation[1] - e_c * eta_c * self.observation[0] + l * observation_[2]
        observation_[1] = self.observation[1] + e_v * k_avg * beta * self.observation[0] * self.observation[1] - e_c * eta_c * self.observation[1] - d * self.observation[1]
        observation_[2] = self.observation[2] + e_c * eta_c * (self.observation[0] + self.observation[1]) - l * observation_[2]
        observation_[3] = self.observation[3] + d * self.observation[1]
    
        reward = -(p_t * self.observation[1] + c_c * (self.observation[0] + self.observation[1]) + kk * self.observation[3] - r * self.observation[2] + m * self.observation[3])
    
        self.observation = observation_
        return observation_, reward, False, "info"

if __name__ == '__main__':
    envi().step(1)
