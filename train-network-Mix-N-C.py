import os
import numpy as np
import argparse
from DDQN import DDQN
from utils import plot_learning_curve, create_directory
from env import envi
import matplotlib.pyplot as plt
plt.style.use('seaborn')

parser = argparse.ArgumentParser()
parser.add_argument('--max_episodes', type=int, default=400)
parser.add_argument('--ckpt_dir', type=str, default='./checkpoints/DDQN/')
parser.add_argument('--reward_path', type=str, default='./img/reward.png')
parser.add_argument('--action_path', type=str, default='./img/action.png')
parser.add_argument('--state_path', type=str, default='./img/state.png')

args = parser.parse_args()


def main():
    env = envi()
    agent = (DDQN(alpha=0.0003, state_dim=env.observation_space, action_dim=env.action_space,
                  fc1_dim=256, fc2_dim=256, ckpt_dir=args.ckpt_dir, gamma=0.99, tau=0.01, epsilon=1.0,
                  eps_end=0.05, eps_dec=5e-4, max_size=1000000, batch_size=256))

    create_directory(args.ckpt_dir, sub_dirs=['Q_eval', 'Q_target'])
    states, rewards, actions, eps_history = [], [], [], []
    cur_state = env.reset()

    for episode in range(args.max_episodes):
        states.append(cur_state)
         # 智能体独自用自己的状态去产生行为
        action = agent.choose_action(cur_state, isTrain=True)
        next_state, reward, done, info = env.stepMix(action)
        agent.remember(cur_state, action, reward, next_state, done)
        agent.learn()
        cur_state = next_state
        #print(np.mean(reward))

        #print(rewar/max_timesteps)
        rewards.append(reward)
        actions.append(action)
        eps_history.append(agent.epsilon)

        if (episode + 1) % 50 == 0:
            agent.save_models(episode + 1)

    episodes = [i for i in range(args.max_episodes)]
    plot_learning_curve(episodes, rewards, 'Reward', 'reward', args.reward_path)
    # plot_learning_curve(episodes, actions, 'Action', 'action', args.action_path)
    # plot_learning_curve(episodes, states, 'Ratio', 'ratio', args.state_path)
    # plot_learning_curve(episodes, actions, 'Energy', 'energy', args.energy_path)

    plt.figure(0)
    observations = np.array(states)
    plt.plot((observations[:, :]))
    # plt.title('Average state distribution mu for each episode')
    plt.xlabel('Time step')
    plt.ylabel('Ratio of devices in different states')
    plt.legend([r'$SL$', r'$IL$', r'$RP$', r'$DL$'], loc='center right')
    plt.savefig('./img/state.png')
    plt.show()

    actionDesc = ['e_v=0,e_c=0,e_n=0','e_v=0,e_c=0,e_n=1','e_v=0,e_c=1,e_n=0','e_v=0,e_c=1,e_n=1','e_v=1,e_c=0,e_n=0','e_v=1,e_c=0,e_n=1','e_v=1,e_c=1,e_n=0','e_v=1,e_c=1,e_n=1']
    actionCount = [0,0,0,0,0,0,0,0]
    print("acCount====")
    print(actionCount)
    for ac in actions:
        print(ac)
        actionCount[ac] = actionCount[ac] + 1

    plt.figure(0)
    for i in range(len(actionCount)):
        plt.bar(i, actionCount[i])
    # plt.title('Average state distribution mu for each episode')
    plt.legend(actionDesc)
    plt.xlabel('Action')
    plt.ylabel('Times')
    plt.savefig('./img/action.png')
    plt.show()

if __name__ == '__main__':
    main()
