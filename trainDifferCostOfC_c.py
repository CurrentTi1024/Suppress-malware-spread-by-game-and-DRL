import os
import numpy as np
import argparse
from DDQN import DDQN
from utils import plot_learning_curve, create_directory
from env import envi
import matplotlib.pyplot as plt
plt.style.use('seaborn')

parser = argparse.ArgumentParser()
parser.add_argument('--max_episodes', type=int, default=200)
parser.add_argument('--ckpt_dir', type=str, default='./checkpoints/DDQN/')
parser.add_argument('--reward_path', type=str, default='./img/reward.png')
parser.add_argument('--action_path', type=str, default='./img/action.png')
parser.add_argument('--state_path', type=str, default='./img/state.png')

args = parser.parse_args()


def main():
    env = envi()
    env2 = envi()
    env3 = envi()
    agent = (DDQN(alpha=0.0003, state_dim=env.observation_space, action_dim=env.action_space,
                  fc1_dim=256, fc2_dim=256, ckpt_dir=args.ckpt_dir, gamma=0.99, tau=0.01, epsilon=1.0,
                  eps_end=0.05, eps_dec=5e-4, max_size=1000000, batch_size=256))
    agent2 = (DDQN(alpha=0.0003, state_dim=env2.observation_space, action_dim=env2.action_space,
                  fc1_dim=256, fc2_dim=256, ckpt_dir=args.ckpt_dir, gamma=0.99, tau=0.01, epsilon=1.0,
                  eps_end=0.05, eps_dec=5e-4, max_size=1000000, batch_size=256))
    agent3 = (DDQN(alpha=0.0003, state_dim=env3.observation_space, action_dim=env3.action_space,
                  fc1_dim=256, fc2_dim=256, ckpt_dir=args.ckpt_dir, gamma=0.99, tau=0.01, epsilon=1.0,
                  eps_end=0.05, eps_dec=5e-4, max_size=1000000, batch_size=256))

    create_directory(args.ckpt_dir, sub_dirs=['Q_eval', 'Q_target'])
    states, rewards, actions, eps_history = [], [], [], []
    states2, rewards2, actions2, eps_history2 = [], [], [], []
    states3, rewards3, actions3, eps_history3 = [], [], [], []
    statesAllOfIL, statesAllOfRP, rewardsOfAll = [], [], []
    cur_state = env.reset()
    cur_state2 = env2.reset()
    cur_state3 = env3.reset()

    for episode in range(args.max_episodes):
        statesOfIL = [cur_state[1],cur_state2[1],cur_state3[1]]
        statesAllOfIL.append(statesOfIL)
        statesOfRP = [cur_state[2],cur_state2[2],cur_state3[2]]
        statesAllOfRP.append(statesOfRP)
        states.append(cur_state)
        states2.append(cur_state2)
        states3.append(cur_state3)
         # 在这里智能体独自用自己的状态去产生行为
        action = agent.choose_action(cur_state, isTrain=True)
        action2 = agent2.choose_action(cur_state2, isTrain=True)
        action3 = agent3.choose_action(cur_state3, isTrain=True)
        next_state, reward, done, info = env.stepMixDifferCost(action, .2, .1, .2, 100, .5, 1.0)
        next_state2, reward2, done2, info2 = env2.stepMixDifferCost(action2, .2, .3, .2, 100, .5, 1.0)
        next_state3, reward3, done3, info3 = env3.stepMixDifferCost(action3, .2, 1.0, .2, 100, .5, 1.0)
        agent.remember(cur_state, action, reward, next_state, done)
        agent2.remember(cur_state2, action2, reward2, next_state2, done2)
        agent3.remember(cur_state3, action3, reward3, next_state3, done3)
        agent.learn()
        agent2.learn()
        agent3.learn()
        cur_state = next_state
        cur_state2 = next_state2
        cur_state3 = next_state3
        
        rewards.append(reward)
        rewards2.append(reward2)
        rewards3.append(reward3)

        rewardsOfPerTime = [reward,reward2,reward3]
        rewardsOfAll.append(rewardsOfPerTime)
        actions.append(action)
        actions2.append(action2)
        actions3.append(action3)
        eps_history.append(agent.epsilon)

        if (episode + 1) % 50 == 0:
            agent.save_models(episode + 1)
            agent2.save_models(episode + 1)
            agent3.save_models(episode + 1)

    episodes = [i for i in range(args.max_episodes)]

    plt.figure(0)
    observations = np.array(statesAllOfIL)
    plt.plot((observations[:, :]))
    plt.xlabel('Time step')
    plt.ylabel('Ratio of ' r'$IL$ ' 'devices with different ' r'$c_c$')
    plt.legend([r'$c_c=0.1$',  r'$c_c=0.3$', r'$c_c=1.0$'], loc='upper right')
    plt.savefig('./img/differCostC_c/stateOfIL.png')
    plt.show()

    plt.figure(0)
    observations = np.array(statesAllOfRP)
    plt.plot((observations[:, :]))
    plt.xlabel('Time step')
    plt.ylabel('Ratio of ' r'$RP$ ' 'devices with different ' r'$c_c$')
    plt.legend([r'$c_c=0.1$',  r'$c_c=0.3$', r'$c_c=1.0$'], loc='lower right')
    plt.savefig('./img/differCostC_c/stateOfRP.png')
    plt.show()

    plt.figure(0)
    observations = np.array(states)
    plt.plot((observations[:, :]))
    plt.xlabel('Time step')
    plt.ylabel('Ratio of devices in different states')
    plt.legend([r'$SL$', r'$IL$', r'$RP$', r'$DL$'], loc='center right')
    plt.savefig('./img/differCostC_c/state.png')
    plt.show()

    plt.figure(0)
    observations = np.array(states2)
    plt.plot((observations[:, :]))
    plt.xlabel('Time step')
    plt.ylabel('Ratio of devices in different states')
    plt.legend([r'$SL$', r'$IL$', r'$RP$', r'$DL$'], loc='center right')
    plt.savefig('./img/differCostC_c/state2.png')
    plt.show()

    plt.figure(0)
    observations = np.array(states3)
    plt.plot((observations[:, :]))
    plt.xlabel('Time step')
    plt.ylabel('Ratio of devices in different states')
    plt.legend([r'$SL$', r'$IL$', r'$RP$', r'$DL$'], loc='center right')
    plt.savefig('./img/differCostC_c/state3.png')
    plt.show()


    plt.figure(0)
    rewardsOfAll = np.array(rewardsOfAll)
    plt.plot((rewardsOfAll[:, :]))
    plt.xlabel('Time step')
    plt.ylabel('Reward for IIoT networks of model ' r'$IIPV$' ' with different ' r'$c_c$')
    plt.legend([r'$c_c=0.1$',  r'$c_c=0.3$', r'$c_c=1.0$'], loc='lower right')
    plt.savefig('./img/differCostC_c/rewardsOfAll.png')
    plt.show()

    
    plt.figure(0)
    rewardsOfAll = np.array(rewardsOfAll)
    plt.plot((rewardsOfAll[:, :]))
    plt.xlabel('Time step')
    plt.ylabel('Reward for IIoT networks of model ' r'$IIPV$' ' with different ' r'$c_c$')
    plt.legend([r'$c_c=0.1$',  r'$c_c=0.3$', r'$c_c=1.0$'], loc='lower right')
    plt.savefig('./img/differCostC_c/rewardsOfAll.png')
    plt.show()

    actionDesc = [r'$e_c$''=0', r'$e_c$''=1', r'$e_n$''=0', r'$e_n$''=1']
    actionCount = [0.0, 0.0, 0.0, 0.0]
    actionCount2 = [0.0, 0.0, 0.0, 0.0]
    actionCount3 = [0.0, 0.0, 0.0, 0.0]
    for i in range(len(actions)):
        if i >= 80:
            # print(actions[i])
            actionCount[(actions[i]>> 1) % 2] = actionCount[(actions[i]>> 1) % 2] + 1/120.0
            actionCount2[(actions2[i]>> 1) % 2] = actionCount2[(actions2[i]>> 1) % 2] + 1/120.0
            actionCount3[(actions3[i]>> 1) % 2] = actionCount3[(actions3[i]>> 1) % 2] + 1/120.0
            actionCount[(actions[i]) % 2 + 2] = actionCount[(actions[i]) % 2 + 2] + 1/120.0
            actionCount2[(actions2[i]) % 2 + 2] = actionCount2[(actions2[i]) % 2 + 2] + 1/120.0
            actionCount3[(actions3[i]) % 2 + 2] = actionCount3[(actions3[i]) % 2 + 2] + 1/120.0

    x = np.arange(len(actionDesc))  # 标签位置
    width = 0.2  # 柱状图的宽度
    fig, ax = plt.subplots()
    rects1 = ax.bar(x - width, actionCount, width, label=r'$c_c=0.1$', hatch="...", color='w', edgecolor="k")
    rects2 = ax.bar(x, actionCount2, width, label=r'$c_c=0.3$', hatch="XX", color='w', edgecolor="k")
    rects3 = ax.bar(x + width, actionCount3, width, label=r'$c_c=1.0$', hatch="++", color='w', edgecolor="k")
    # 为y轴、标题和x轴等添加一些文本。
    ax.set_ylabel('Statistics of actions', fontsize=12)
    ax.set_xlabel('Action ' r'$e_c$ and '  r'$e_n$', fontsize=12)
    # ax.set_title('标题')
    ax.set_xticks(x)
    ax.set_xticklabels(actionDesc)
    ax.legend()
    plt.savefig('./img/differCostC_c/actionOfAll.png')
    plt.show()

if __name__ == '__main__':
    main()
